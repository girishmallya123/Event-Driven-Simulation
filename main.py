import logging
from config_reader import Config
import pareto_sampler
from exceptions import InvalidParameterException, InvalidCachingPolicyException
import sys
import numpy as np
import poisson_process
from file import File
from event import Event
from fifo_queue import FifoQueue
from lifo_queue import LifoQueue
from lru_cache import LRUCache
from lfu_cache import LFUCache
from mfu_cache import MFUCache
from no_cache import NoCache
import constants
import pandas as pd

def run_simulation(cache_policies, request_events, params):
    avg_turnaround_times = []
    for policy in cache_policies:
        logging.info("run_simulation# Simulating for {} policy".format(policy))
        cache = None

        if policy == "FIFO":
            cache = FifoQueue(params.cache_capacity)
        elif policy == "LIFO":
            cache = LifoQueue(params.cache_capacity)
        elif policy == "LRU":
            cache = LRUCache(params.cache_capacity)
        elif policy == "LFU":
            cache = LFUCache(params.cache_capacity)
        elif policy == "MFU":
            cache = MFUCache(params.cache_capacity)
        elif policy == "NO_CACHING":
            cache = NoCache(params.cache_capacity)
        else:
            raise InvalidCachingPolicyException

        current_time = request_events[0].req_file.arrival_time
        for i, event in enumerate(request_events):
            cache_hit = cache.find(event.req_file)
            if cache_hit:
                event.finish_time = current_time + float(event.req_file.file_size / params.cache_transmission_rate) 
            else:
                cache.insert(event.req_file)
                event.finish_time = current_time + constants.RTT + float(event.req_file.file_size / params.network_transmission_rate) + + float(event.req_file.file_size / params.cache_transmission_rate)

            if i < len(request_events)-1:
                current_time = max(event.finish_time, request_events[i+1].req_file.arrival_time)
            else:
                current_time = event.finish_time
        avg_turnaround_time = sum((event.finish_time - event.req_file.arrival_time) for event in request_events) / len(request_events)
        avg_turnaround_times.append(avg_turnaround_time)
            
    return avg_turnaround_times

def main():

    '''logging'''
    logging.basicConfig(filename='event.log', level=logging.INFO)
    logging.info("main#Starting the simulator")
    params = Config()

    try:
        file_sizes = pareto_sampler.generate_pareto_samples(params, mode="size")
        file_q = pareto_sampler.generate_pareto_samples(params, mode="popularity")
        file_pop = file_q / sum(file_q)
    except InvalidParameterException:
        logging.error("main# Exitting because the parameters are invalid to generate a pareto distribution of file sizes")
        sys.exit(0)

    '''
        the number of events in this case, files requested are generated using
        the Poisson Process.
    '''
    poisson_lambda = params.poisson_lambda
    number_of_reqs = params.number_of_requests

    '''
        create instances of events for each time generated by the Poisson Distribution. We randomly select a file
        that would be requested, this is done by taking the file popularities into account. 
    '''

    request_events = []
    event_counter = poisson_process.poisson_process(poisson_lambda, number_of_reqs)

    logging.info("main# Randomly generating {} file request events".format(number_of_reqs))
    for event in event_counter:
        _file_arrival_time = event
        random_file_ix = np.random.choice(params.N, size=1, replace=True, p= file_pop)
        _file = File(random_file_ix[0], file_sizes[random_file_ix], _file_arrival_time)
        _event = Event(_file)
        request_events.append(_event)

    cache_policies = ['LIFO', 'FIFO', 'LRU', 'LFU', 'MFU', 'NO_CACHING']

    results = run_simulation(cache_policies, request_events, params)
    results_df = {}
    results_df['policies'] = cache_policies
    results_df['mean_turnaround_times'] = results
    
    df = pd.DataFrame(results_df)
    df.to_csv("results.csv", index=False)
if __name__ == "__main__":
    main()
